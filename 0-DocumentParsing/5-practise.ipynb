{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eab5a9e",
   "metadata": {},
   "source": [
    "### This is practise for all things learnt till now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176faafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "#print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34232be",
   "metadata": {},
   "source": [
    "### Load the pdf file and split the documents into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac6924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "__loader = PyPDFLoader(\"data/pdf/AI Agents guidebook.pdf\")\n",
    "__documents = __loader.load()\n",
    "print(f\"Total Pages in PDF: {len(__documents)}\")\n",
    "__documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da60ab",
   "metadata": {},
   "source": [
    "### Split the documents into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d7c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_spiltter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "docs = text_spiltter.split_documents(__documents)\n",
    "print(type(docs[0]))\n",
    "print(f\"Total Pages in PDF: {len(__documents)}\")\n",
    "print('-------------------------------')\n",
    "print(f\"Total Chunks created: {len(docs)}\")\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af80d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Initialize OpenAI embeddings\n",
    "openai_embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Extract text content from Document objects - this was the bug!\n",
    "# embed_documents expects list of strings, not Document objects\n",
    "texts = [doc.page_content for doc in docs]\n",
    "\n",
    "# Now embed the text content\n",
    "openai_vector = openai_embeddings.embed_documents(texts)\n",
    "\n",
    "# Print results\n",
    "print(\"âœ… OpenAI Document Embeddings created successfully!\")\n",
    "print(f'Number of documents embedded: {len(openai_vector)}')\n",
    "print(f'Embedding dimension: {len(openai_vector[0])}')\n",
    "print(f'First 5 values of first document embedding: {openai_vector[0][:5]}')\n",
    "print('---------------------------')\n",
    "print(f'First 5 values of second document embedding: {openai_vector[1][:5]}')\n",
    "print('\\nOriginal document sample:')\n",
    "print(f'First doc content preview: {texts[0][:100]}...')\n",
    "\n",
    "# Verify we have the right data types\n",
    "print(f'\\nData type verification:')\n",
    "print(f'Type of docs[0]: {type(docs[0])}')\n",
    "print(f'Type of texts[0]: {type(texts[0])}')\n",
    "print(f'docs[0] has page_content: {hasattr(docs[0], \"page_content\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c76d8d",
   "metadata": {},
   "source": [
    "### Embed the chunks and create the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50ef0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, openai_embeddings)\n",
    "query = \"What is an AI agent?\"\n",
    "print(f\"Top 2 most similar documents to '{query}':\")\n",
    "results = vectorstore.similarity_search_with_score(query, k=2)\n",
    "for i, (doc, score) in enumerate(results):\n",
    "    print(f\"\\nDocument {i+1} (Score: {score}):\\n{doc.page_content[:100]}...\")\n",
    "\n",
    "chromastore = Chroma.from_documents(docs, openai_embeddings, collection_name=\"ai-agents-guidebook\")\n",
    "results = chromastore.similarity_search_with_score(query, k=2)\n",
    "for i, (doc, score) in enumerate(results):\n",
    "    print(f\"\\nDocument {i+1} (Score: {score}):\\n{doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea9de65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and persist the Chroma vector store\n",
    "persist_directory = \"data/chroma\"  # Choose your directory\n",
    "chromastore = Chroma.from_documents(docs, openai_embeddings, persist_directory=persist_directory)\n",
    "\n",
    "# Save (persist) the vector store to disk\n",
    "chromastore.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reuse_embeddings_intro",
   "metadata": {},
   "source": [
    "## ğŸ”¥ Reusing Pre-computed Embeddings with Vector Stores\n",
    "\n",
    "Now that we have our embeddings computed, let's see how to **reuse** them with different vector stores.\n",
    "\n",
    "**ğŸ’¡ Key Insight:** We already spent time and money computing embeddings using `embed_documents()`.\n",
    "Instead of letting FAISS/Chroma recompute embeddings (which costs time and API calls), we'll reuse our existing embeddings!\n",
    "\n",
    "**Benefits:**\n",
    "- ğŸ’° Save money on API calls\n",
    "- âš¡ Faster vector store creation\n",
    "- ğŸ”„ Consistent embeddings across different vector stores\n",
    "- ğŸ§ª Easy experimentation with different vector DBs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faiss_with_precomputed_embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”¥ METHOD 1: FAISS with Pre-computed Embeddings\n",
    "# ================================================\n",
    "\n",
    "import numpy as np\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "print('ğŸš€ Creating FAISS vector store with PRE-COMPUTED embeddings...')\n",
    "print(f'ğŸ“Š We have {len(openai_vector)} embeddings of dimension {len(openai_vector[0])}')\n",
    "print()\n",
    "\n",
    "# Convert embeddings to numpy array (FAISS requirement)\n",
    "embedding_matrix = np.array(openai_vector, dtype=np.float32)\n",
    "print(f'ğŸ“Š Embedding matrix shape: {embedding_matrix.shape}')\n",
    "print(f'ğŸ“Š Data type: {embedding_matrix.dtype}')\n",
    "print()\n",
    "\n",
    "# ğŸ”‘ KEY: Create FAISS index directly from pre-computed embeddings\n",
    "# We use from_embeddings() method - this is the secret sauce!\n",
    "faiss_vectorstore = FAISS.from_embeddings(\n",
    "    text_embeddings=list(zip(texts, openai_vector)),  # Our pre-computed embeddings!\n",
    "    embedding=openai_embeddings,  # The embedding model (for future queries)\n",
    "    metadatas=[doc.metadata for doc in docs]  # Include metadata from original docs\n",
    ")\n",
    "\n",
    "print('âœ… FAISS vectorstore created using PRE-COMPUTED embeddings!')\n",
    "print(f'ğŸ“Š Number of documents in FAISS index: {faiss_vectorstore.index.ntotal}')\n",
    "print()\n",
    "\n",
    "# Test similarity search with FAISS\n",
    "query = \"What are AI agents and how do they work?\"\n",
    "print(f'ğŸ” Testing FAISS search with query: \"{query}\"')\n",
    "\n",
    "similar_docs = faiss_vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print('ğŸ” Top 3 similar documents from FAISS:')\n",
    "for i, doc in enumerate(similar_docs, 1):\n",
    "    print(f'  {i}. {doc.page_content[:120]}...')\n",
    "    print(f'     ğŸ“„ Source: {doc.metadata.get(\"source\", \"N/A\")}')\n",
    "    print(f'     ğŸ“– Page: {doc.metadata.get(\"page\", \"N/A\")}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chroma_with_precomputed_embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”¥ METHOD 2: Chroma with Pre-computed Embeddings\n",
    "# ===============================================\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "print('ğŸš€ Creating Chroma vector store with PRE-COMPUTED embeddings...')\n",
    "print()\n",
    "\n",
    "# Create a directory for Chroma database\n",
    "chroma_db_path = './chroma_db_precomputed'\n",
    "\n",
    "# ğŸ”‘ KEY: Create Chroma vectorstore using pre-computed embeddings\n",
    "# Method A: Using from_embeddings (similar to FAISS)\n",
    "chroma_vectorstore_precomputed = Chroma.from_embeddings(\n",
    "    embeddings=list(zip(texts, openai_vector)),  # Our pre-computed embeddings!\n",
    "    embedding=openai_embeddings,  # The embedding model (for future queries)\n",
    "    metadatas=[doc.metadata for doc in docs],  # Include metadata\n",
    "    collection_name=\"ai_agents_precomputed\",\n",
    "    persist_directory=chroma_db_path\n",
    ")\n",
    "\n",
    "print('âœ… Chroma vectorstore created using PRE-COMPUTED embeddings!')\n",
    "print(f'ğŸ“Š Collection name: ai_agents_precomputed')\n",
    "print(f'ğŸ“ Persist directory: {chroma_db_path}')\n",
    "print()\n",
    "\n",
    "# Test similarity search with Chroma\n",
    "query = \"What are the building blocks of AI agents?\"\n",
    "print(f'ğŸ” Testing Chroma search with query: \"{query}\"')\n",
    "\n",
    "chroma_results = chroma_vectorstore_precomputed.similarity_search(query, k=3)\n",
    "\n",
    "print('ğŸ” Top 3 similar documents from Chroma:')\n",
    "for i, doc in enumerate(chroma_results, 1):\n",
    "    print(f'  {i}. {doc.page_content[:120]}...')\n",
    "    print(f'     ğŸ“„ Source: {doc.metadata.get(\"source\", \"N/A\")}')\n",
    "    print(f'     ğŸ“– Page: {doc.metadata.get(\"page\", \"N/A\")}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance_comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š PERFORMANCE COMPARISON & BENEFITS ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "import time\n",
    "\n",
    "print('ğŸš€ PERFORMANCE BENEFITS OF REUSING EMBEDDINGS')\n",
    "print('=' * 55)\n",
    "print()\n",
    "\n",
    "# Calculate actual metrics\n",
    "num_documents = len(docs)\n",
    "embedding_dim = len(openai_vector[0])\n",
    "\n",
    "# Cost analysis (approximate OpenAI pricing)\n",
    "cost_per_1k_tokens = 0.0001  # text-embedding-ada-002 pricing\n",
    "avg_tokens_per_doc = 250  # Rough estimate\n",
    "total_tokens = (num_documents * avg_tokens_per_doc) / 1000\n",
    "estimated_cost = total_tokens * cost_per_1k_tokens\n",
    "\n",
    "print('ğŸ’° COST ANALYSIS:')\n",
    "print(f'   ğŸ“Š Documents processed: {num_documents}')\n",
    "print(f'   ğŸ“Š Embedding dimension: {embedding_dim}')\n",
    "print(f'   ğŸ“Š Estimated tokens: {total_tokens:.1f}k')\n",
    "print(f'   ğŸ’¸ Cost to embed once: ${estimated_cost:.4f}')\n",
    "print(f'   ğŸ’¸ Cost if we re-embed for each vector store: ${estimated_cost:.4f}')\n",
    "print(f'   âœ… Money saved by reusing: ${estimated_cost:.4f} per additional vector store')\n",
    "print()\n",
    "\n",
    "print('âš¡ TIME ANALYSIS:')\n",
    "print(f'   ğŸ• Time to embed {num_documents} docs: ~30-60 seconds (API calls)')\n",
    "print(f'   âš¡ Time to reuse embeddings: ~1-2 seconds (no API calls)')\n",
    "print(f'   âœ… Time saved: ~28-58 seconds per vector store')\n",
    "print()\n",
    "\n",
    "print('ğŸ¯ KEY ADVANTAGES:')\n",
    "advantages = [\n",
    "    'No repeated API calls to OpenAI',\n",
    "    'Consistent embeddings across different vector stores',\n",
    "    'Faster experimentation with different vector DBs',\n",
    "    'Cost-effective for multiple vector store comparisons',\n",
    "    'Better for production workflows with caching',\n",
    "    'Reduced rate limiting issues',\n",
    "    'Offline capability once embeddings are cached'\n",
    "]\n",
    "\n",
    "for advantage in advantages:\n",
    "    print(f'   âœ… {advantage}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "search_performance_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” SEARCH PERFORMANCE TEST\n",
    "# =========================\n",
    "\n",
    "import time\n",
    "\n",
    "# Test query\n",
    "test_query = \"AI agent tools and capabilities\"\n",
    "\n",
    "print(f'ğŸ” SEARCH PERFORMANCE TEST')\n",
    "print(f'Query: \"{test_query}\"')\n",
    "print('-' * 50)\n",
    "print()\n",
    "\n",
    "# Test FAISS search performance\n",
    "print('ğŸš€ Testing FAISS search...')\n",
    "start_time = time.time()\n",
    "faiss_results = faiss_vectorstore.similarity_search(test_query, k=2)\n",
    "faiss_time = time.time() - start_time\n",
    "\n",
    "# Test Chroma search performance\n",
    "print('ğŸš€ Testing Chroma search...')\n",
    "start_time = time.time()\n",
    "chroma_results = chroma_vectorstore_precomputed.similarity_search(test_query, k=2)\n",
    "chroma_time = time.time() - start_time\n",
    "\n",
    "print('âš¡ PERFORMANCE RESULTS:')\n",
    "print(f'   FAISS search time: {faiss_time:.4f} seconds')\n",
    "print(f'   Chroma search time: {chroma_time:.4f} seconds')\n",
    "print()\n",
    "\n",
    "print('ğŸ” FAISS Results:')\n",
    "for i, doc in enumerate(faiss_results, 1):\n",
    "    print(f'  {i}. {doc.page_content[:100]}...')\n",
    "\n",
    "print('ğŸ” Chroma Results:')\n",
    "for i, doc in enumerate(chroma_results, 1):\n",
    "    print(f'  {i}. {doc.page_content[:100]}...')\n",
    "\n",
    "print('ğŸ‰ SUCCESS! Both vector stores are using the SAME pre-computed embeddings!')\n",
    "print('ğŸ¯ Notice how both return semantically similar results because they use identical embeddings.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_reusing_embeddings",
   "metadata": {},
   "source": [
    "## ğŸ¯ Summary: Reusing Pre-computed Embeddings\n",
    "\n",
    "We've successfully demonstrated methods to reuse pre-computed embeddings:\n",
    "\n",
    "### ğŸ”‘ Key Methods:\n",
    "\n",
    "#### 1. **FAISS with Pre-computed Embeddings**\n",
    "```python\n",
    "FAISS.from_embeddings(\n",
    "    text_embeddings=list(zip(texts, your_embeddings)),\n",
    "    embedding=embedding_model,\n",
    "    metadatas=metadata_list\n",
    ")\n",
    "```\n",
    "\n",
    "#### 2. **Chroma with Pre-computed Embeddings**\n",
    "```python\n",
    "Chroma.from_embeddings(\n",
    "    embeddings=list(zip(texts, your_embeddings)),\n",
    "    embedding=embedding_model,\n",
    "    metadatas=metadata_list\n",
    ")\n",
    "```\n",
    "\n",
    "### ğŸ’° Benefits Achieved:\n",
    "\n",
    "- **ğŸ’¸ Cost Savings**: No repeated API calls to OpenAI\n",
    "- **âš¡ Speed**: Vector store creation in seconds vs minutes\n",
    "- **ğŸ”„ Consistency**: Same embeddings across different vector stores\n",
    "- **ğŸ§ª Easy Experimentation**: Test different vector DBs without re-embedding\n",
    "\n",
    "### ğŸ¯ When to Use This Approach:\n",
    "\n",
    "- âœ… **Comparing vector stores** (FAISS vs Chroma vs Pinecone)\n",
    "- âœ… **Production workflows** where you cache embeddings\n",
    "- âœ… **Cost-sensitive applications**\n",
    "- âœ… **Development/experimentation** with different vector DBs\n",
    "- âœ… **When you need consistent embedding versions across deployments**\n",
    "\n",
    "### ğŸš€ Next Steps:\n",
    "\n",
    "Now you can:\n",
    "1. Save your `openai_vector` embeddings to disk for future use\n",
    "2. Load them in different notebooks/applications\n",
    "3. Create multiple vector stores instantly\n",
    "4. Compare search performance across different vector databases\n",
    "5. Deploy to production with cached embeddings for better performance!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGUdemy (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}