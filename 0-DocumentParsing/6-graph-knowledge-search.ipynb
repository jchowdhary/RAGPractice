{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph-based Knowledge Graph with Hybrid Search\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Building a **Graph-based Knowledge Graph** from documents\n",
    "2. **Graph-based search** using relationships and entities\n",
    "3. **Hybrid approach** combining Graph KG with embedding-based search\n",
    "4. **Performance comparison** between different search methods\n",
    "\n",
    "## Why Graph Knowledge Graphs?\n",
    "- **Relationships**: Capture explicit relationships between entities\n",
    "- **Reasoning**: Enable complex queries and inference\n",
    "- **Structure**: Organized knowledge representation\n",
    "- **Complementary**: Works well with embedding-based semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: networkx in /var/data/python/lib/python3.12/site-packages (3.5)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.6-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.56.1-py3-none-any.whl.metadata (42 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.60.0-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (111 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /var/data/python/lib/python3.12/site-packages (from matplotlib) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /var/data/python/lib/python3.12/site-packages (from matplotlib) (11.3.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.4-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /var/data/python/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.13-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.10-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /var/data/python/lib/python3.12/site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /var/data/python/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /var/data/python/lib/python3.12/site-packages (from spacy) (2.32.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /var/data/python/lib/python3.12/site-packages (from spacy) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in /var/data/python/lib/python3.12/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3.12/site-packages (from spacy) (80.9.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /var/data/python/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /var/data/python/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /var/data/python/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /var/data/python/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /var/data/python/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /var/data/python/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /var/data/python/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /var/data/python/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.7.14)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /var/data/python/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /var/data/python/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /var/data/python/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.1.0)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.22.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.3.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: filelock in /var/data/python/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /var/data/python/lib/python3.12/site-packages (from transformers) (0.34.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /var/data/python/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /var/data/python/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /var/data/python/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /var/data/python/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.16.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.3.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /var/data/python/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /var/data/python/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /var/data/python/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.4.0 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /var/data/python/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading matplotlib-3.10.6-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading spacy-3.8.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m33.9/33.9 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (227 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.13-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
      "Downloading preshed-3.0.10-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (869 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m869.3/869.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.3.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading blis-1.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading cloudpathlib-0.22.0-py3-none-any.whl (61 kB)\n",
      "Downloading smart_open-7.3.1-py3-none-any.whl (61 kB)\n",
      "Downloading transformers-4.56.1-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Downloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.60.0-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading marisa_trie-1.3.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.4-py3-none-any.whl (113 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)\n",
      "\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”\u001b[0m \u001b[32m766.5/887.9 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:14\u001b[0mm\n",
      "\u001b[?25h\u001b[31mERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langchain\n",
      "  Using cached langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: chromadb in /var/data/python/lib/python3.12/site-packages (1.0.15)\n",
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: openai in /var/data/python/lib/python3.12/site-packages (1.97.1)\n",
      "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n",
      "  Downloading langchain_core-0.3.76-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting langsmith>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.4.28-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /var/data/python/lib/python3.12/site-packages (from langchain) (2.11.7)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading sqlalchemy-2.0.43-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /var/data/python/lib/python3.12/site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /var/data/python/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /var/data/python/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.72->langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /var/data/python/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in /usr/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /var/data/python/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /var/data/python/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /var/data/python/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /var/data/python/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /var/data/python/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /var/data/python/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /var/data/python/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2025.7.14)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.2.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: build>=1.0.3 in /var/data/python/lib/python3.12/site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in /var/data/python/lib/python3.12/site-packages (from chromadb) (1.4.1)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /var/data/python/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /var/data/python/lib/python3.12/site-packages (from chromadb) (2.3.2)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /var/data/python/lib/python3.12/site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /var/data/python/lib/python3.12/site-packages (from chromadb) (1.22.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /var/data/python/lib/python3.12/site-packages (from chromadb) (1.35.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /var/data/python/lib/python3.12/site-packages (from chromadb) (1.35.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /var/data/python/lib/python3.12/site-packages (from chromadb) (1.35.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /var/data/python/lib/python3.12/site-packages (from chromadb) (0.21.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /var/data/python/lib/python3.12/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /var/data/python/lib/python3.12/site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /var/data/python/lib/python3.12/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /var/data/python/lib/python3.12/site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /var/data/python/lib/python3.12/site-packages (from chromadb) (1.74.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /var/data/python/lib/python3.12/site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /var/data/python/lib/python3.12/site-packages (from chromadb) (0.16.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /var/data/python/lib/python3.12/site-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /var/data/python/lib/python3.12/site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /var/data/python/lib/python3.12/site-packages (from chromadb) (3.11.1)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /var/data/python/lib/python3.12/site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /var/data/python/lib/python3.12/site-packages (from chromadb) (14.1.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /var/data/python/lib/python3.12/site-packages (from chromadb) (4.25.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in /var/data/python/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /var/data/python/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /var/data/python/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /var/data/python/lib/python3.12/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /var/data/python/lib/python3.12/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /var/data/python/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in /var/data/python/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /var/data/python/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: pyproject_hooks in /var/data/python/lib/python3.12/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /var/data/python/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /var/data/python/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /var/data/python/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /var/data/python/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (0.26.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /var/data/python/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /var/data/python/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /var/data/python/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /var/data/python/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: durationpy>=0.7 in /var/data/python/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /var/data/python/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /var/data/python/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /var/data/python/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /var/data/python/lib/python3.12/site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith>=0.1.17->langchain)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: coloredlogs in /var/data/python/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /var/data/python/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /var/data/python/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (6.31.1)\n",
      "Requirement already satisfied: sympy in /var/data/python/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /var/data/python/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /var/data/python/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /var/data/python/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.35.0 in /var/data/python/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.35.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.35.0 in /var/data/python/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.35.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.56b0 in /var/data/python/lib/python3.12/site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.56b0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /var/data/python/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /var/data/python/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /var/data/python/lib/python3.12/site-packages (from tokenizers>=0.13.2->chromadb) (0.34.1)\n",
      "Requirement already satisfied: filelock in /var/data/python/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /var/data/python/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.7.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /var/data/python/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /var/data/python/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /var/data/python/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /var/data/python/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /var/data/python/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /var/data/python/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /var/data/python/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /var/data/python/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /var/data/python/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /var/data/python/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Using cached langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "Downloading langchain_core-0.3.76-py3-none-any.whl (447 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_text_splitters-0.3.11-py3-none-any.whl (33 kB)\n",
      "Downloading sqlalchemy-2.0.43-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading greenlet-3.2.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (607 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m607.6/607.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading langsmith-0.4.28-py3-none-any.whl (384 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading zstandard-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: zstandard, jsonpointer, greenlet, faiss-cpu, SQLAlchemy, requests-toolbelt, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11/11\u001b[0m [langchain]11\u001b[0m [langchain]text-splitters]\n",
      "\u001b[1A\u001b[2KSuccessfully installed SQLAlchemy-2.0.43 faiss-cpu-1.12.0 greenlet-3.2.4 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.27 langchain-core-0.3.76 langchain-text-splitters-0.3.11 langsmith-0.4.28 requests-toolbelt-1.0.0 zstandard-0.25.0\n",
      "/home/jayant/KrishAcademy/RAGUdemy/.venv/bin/python: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install networkx matplotlib spacy transformers sentence-transformers\n",
    "!pip install langchain chromadb faiss-cpu openai\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libbz2.so.1.0: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnetworkx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnx\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KrishAcademy/RAGUdemy/.venv/lib/python3.11/site-packages/networkx/__init__.py:19\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnetworkx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlazy_imports\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _lazy_import\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnetworkx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnetworkx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnetworkx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _clear_cache, _dispatchable\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# load_and_call entry_points, set configs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KrishAcademy/RAGUdemy/.venv/lib/python3.11/site-packages/networkx/utils/__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnetworkx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmisc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnetworkx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecorators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnetworkx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrandom_sequence\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnetworkx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01munion_find\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KrishAcademy/RAGUdemy/.venv/lib/python3.11/site-packages/networkx/utils/decorators.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbz2\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgzip\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.4/lib/python3.11/bz2.py:17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_compression\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_bz2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BZ2Compressor, BZ2Decompressor\n\u001b[32m     20\u001b[39m _MODE_CLOSED   = \u001b[32m0\u001b[39m\n\u001b[32m     21\u001b[39m _MODE_READ     = \u001b[32m1\u001b[39m\n",
      "\u001b[31mImportError\u001b[39m: libbz2.so.1.0: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Set\n",
    "import time\n",
    "\n",
    "# Embedding and vector store imports\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from langchain.vectorstores import FAISS, Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load spaCy model for NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(\"âœ… All packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Document Data\n",
    "\n",
    "Let's create some sample documents that contain entities and relationships we can extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents with rich entity relationships\n",
    "sample_documents = [\n",
    "    {\n",
    "        \"id\": \"doc_1\",\n",
    "        \"title\": \"Introduction to Machine Learning\",\n",
    "        \"content\": \"Machine Learning is a subset of Artificial Intelligence that enables computers to learn without being explicitly programmed. Python is widely used for ML development. Popular libraries include TensorFlow, PyTorch, and Scikit-learn. Supervised learning uses labeled data to train models.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_2\",\n",
    "        \"title\": \"Deep Learning Fundamentals\",\n",
    "        \"content\": \"Deep Learning is a subset of Machine Learning that uses neural networks with multiple layers. TensorFlow and PyTorch are the most popular frameworks. Deep Learning excels in computer vision and natural language processing. CUDA enables GPU acceleration for faster training.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_3\",\n",
    "        \"title\": \"Python for Data Science\",\n",
    "        \"content\": \"Python is the preferred programming language for data science and machine learning. Key libraries include NumPy for numerical computing, Pandas for data manipulation, and Matplotlib for visualization. Jupyter notebooks provide an interactive development environment.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_4\",\n",
    "        \"title\": \"Natural Language Processing\",\n",
    "        \"content\": \"Natural Language Processing (NLP) is a field of Artificial Intelligence focused on understanding human language. Transformers revolutionized NLP with models like BERT and GPT. PyTorch and TensorFlow support NLP model development. Applications include sentiment analysis and machine translation.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_5\",\n",
    "        \"title\": \"Computer Vision Applications\",\n",
    "        \"content\": \"Computer Vision enables machines to interpret visual information. Deep Learning techniques, particularly Convolutional Neural Networks (CNNs), are essential. OpenCV provides computer vision tools, while TensorFlow and PyTorch offer deep learning frameworks. Applications include image recognition and object detection.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_6\",\n",
    "        \"title\": \"Data Visualization with Python\",\n",
    "        \"content\": \"Data visualization is crucial for data analysis and presentation. Matplotlib is the foundational plotting library in Python. Seaborn provides statistical visualizations, while Plotly enables interactive charts. Jupyter notebooks integrate well with visualization libraries for exploratory data analysis.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"ğŸ“š Created {len(sample_documents)} sample documents\")\n",
    "for doc in sample_documents:\n",
    "    print(f\"  - {doc['title']} ({len(doc['content'])} chars)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Entity Extraction and Relationship Building\n",
    "\n",
    "We'll extract entities and relationships from documents to build our knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeGraphBuilder:\n",
    "    def __init__(self):\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.entities = set()\n",
    "        self.relationships = []\n",
    "        self.entity_documents = defaultdict(list)  # Track which documents contain which entities\n",
    "        \n",
    "    def extract_entities(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract named entities using spaCy NER\"\"\"\n",
    "        doc = nlp(text)\n",
    "        entities = []\n",
    "        \n",
    "        # Extract named entities\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in ['ORG', 'PRODUCT', 'LANGUAGE', 'PERSON', 'TECHNOLOGY']:\n",
    "                entities.append(ent.text)\n",
    "        \n",
    "        # Extract key technical terms (simple keyword extraction)\n",
    "        tech_terms = [\n",
    "            'Machine Learning', 'Deep Learning', 'Artificial Intelligence', 'AI',\n",
    "            'Python', 'TensorFlow', 'PyTorch', 'Scikit-learn', 'NumPy', 'Pandas',\n",
    "            'Matplotlib', 'Seaborn', 'Plotly', 'Jupyter', 'OpenCV', 'CUDA',\n",
    "            'Neural Networks', 'CNN', 'BERT', 'GPT', 'Transformers',\n",
    "            'Computer Vision', 'Natural Language Processing', 'NLP',\n",
    "            'Supervised Learning', 'Data Science', 'Data Visualization'\n",
    "        ]\n",
    "        \n",
    "        for term in tech_terms:\n",
    "            if term.lower() in text.lower():\n",
    "                entities.append(term)\n",
    "        \n",
    "        return list(set(entities))  # Remove duplicates\n",
    "    \n",
    "    def extract_relationships(self, text: str, entities: List[str]) -> List[Tuple[str, str, str]]:\n",
    "        \"\"\"Extract relationships between entities\"\"\"\n",
    "        relationships = []\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Define relationship patterns\n",
    "        patterns = [\n",
    "            ('subset_of', ['is a subset of', 'subset of']),\n",
    "            ('uses', ['uses', 'utilize', 'employs']),\n",
    "            ('includes', ['includes', 'include', 'contains']),\n",
    "            ('enables', ['enables', 'enable', 'allows']),\n",
    "            ('supports', ['supports', 'support']),\n",
    "            ('provides', ['provides', 'provide', 'offers']),\n",
    "            ('excels_in', ['excels in', 'good for']),\n",
    "            ('revolutionized', ['revolutionized', 'transformed']),\n",
    "            ('integrates_with', ['integrate', 'integrates with'])\n",
    "        ]\n",
    "        \n",
    "        # Find relationships between entities\n",
    "        for i, entity1 in enumerate(entities):\n",
    "            for j, entity2 in enumerate(entities):\n",
    "                if i != j:\n",
    "                    entity1_lower = entity1.lower()\n",
    "                    entity2_lower = entity2.lower()\n",
    "                    \n",
    "                    # Check if both entities appear in the same sentence\n",
    "                    sentences = text.split('.')\n",
    "                    for sentence in sentences:\n",
    "                        sentence_lower = sentence.lower()\n",
    "                        if entity1_lower in sentence_lower and entity2_lower in sentence_lower:\n",
    "                            # Check for relationship patterns\n",
    "                            for rel_type, keywords in patterns:\n",
    "                                for keyword in keywords:\n",
    "                                    if keyword in sentence_lower:\n",
    "                                        # Determine direction based on sentence structure\n",
    "                                        entity1_pos = sentence_lower.find(entity1_lower)\n",
    "                                        entity2_pos = sentence_lower.find(entity2_lower)\n",
    "                                        keyword_pos = sentence_lower.find(keyword)\n",
    "                                        \n",
    "                                        # Simple heuristic: if keyword is between entities\n",
    "                                        if entity1_pos < keyword_pos < entity2_pos:\n",
    "                                            relationships.append((entity1, rel_type, entity2))\n",
    "                                        elif entity2_pos < keyword_pos < entity1_pos:\n",
    "                                            relationships.append((entity2, rel_type, entity1))\n",
    "        \n",
    "        return relationships\n",
    "    \n",
    "    def build_graph(self, documents: List[Dict]) -> None:\n",
    "        \"\"\"Build knowledge graph from documents\"\"\"\n",
    "        for doc in documents:\n",
    "            text = doc['title'] + ' ' + doc['content']\n",
    "            \n",
    "            # Extract entities\n",
    "            entities = self.extract_entities(text)\n",
    "            \n",
    "            # Add entities to graph\n",
    "            for entity in entities:\n",
    "                self.graph.add_node(entity, type='entity')\n",
    "                self.entities.add(entity)\n",
    "                self.entity_documents[entity].append(doc['id'])\n",
    "            \n",
    "            # Extract and add relationships\n",
    "            relationships = self.extract_relationships(text, entities)\n",
    "            for subj, pred, obj in relationships:\n",
    "                self.graph.add_edge(subj, obj, relation=pred, source_doc=doc['id'])\n",
    "                self.relationships.append((subj, pred, obj, doc['id']))\n",
    "    \n",
    "    def get_entity_info(self, entity: str) -> Dict:\n",
    "        \"\"\"Get information about an entity\"\"\"\n",
    "        if entity not in self.entities:\n",
    "            return {}\n",
    "        \n",
    "        # Get connections\n",
    "        outgoing = list(self.graph.successors(entity))\n",
    "        incoming = list(self.graph.predecessors(entity))\n",
    "        \n",
    "        # Get relationship details\n",
    "        relationships = []\n",
    "        for neighbor in outgoing:\n",
    "            edge_data = self.graph[entity][neighbor]\n",
    "            relationships.append({\n",
    "                'type': 'outgoing',\n",
    "                'target': neighbor,\n",
    "                'relation': edge_data.get('relation', 'related_to'),\n",
    "                'source_doc': edge_data.get('source_doc', 'unknown')\n",
    "            })\n",
    "        \n",
    "        for neighbor in incoming:\n",
    "            edge_data = self.graph[neighbor][entity]\n",
    "            relationships.append({\n",
    "                'type': 'incoming',\n",
    "                'source': neighbor,\n",
    "                'relation': edge_data.get('relation', 'related_to'),\n",
    "                'source_doc': edge_data.get('source_doc', 'unknown')\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'entity': entity,\n",
    "            'documents': self.entity_documents[entity],\n",
    "            'connections': len(outgoing) + len(incoming),\n",
    "            'relationships': relationships\n",
    "        }\n",
    "\n",
    "# Build the knowledge graph\n",
    "kg_builder = KnowledgeGraphBuilder()\n",
    "kg_builder.build_graph(sample_documents)\n",
    "\n",
    "print(f\"ğŸ•¸ï¸ Knowledge Graph built successfully!\")\n",
    "print(f\"   Entities: {len(kg_builder.entities)}\")\n",
    "print(f\"   Relationships: {len(kg_builder.relationships)}\")\n",
    "print(f\"   Graph edges: {kg_builder.graph.number_of_edges()}\")\n",
    "\n",
    "# Show some entities\n",
    "print(f\"\\nğŸ“‹ Sample entities:\")\n",
    "for entity in list(kg_builder.entities)[:10]:\n",
    "    print(f\"   - {entity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Graph Visualization\n",
    "\n",
    "Let's visualize our knowledge graph to understand the entity relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_knowledge_graph(kg_builder, max_nodes=20, figsize=(15, 10)):\n",
    "    \"\"\"Visualize the knowledge graph\"\"\"\n",
    "    # Get subgraph with most connected nodes\n",
    "    node_degrees = dict(kg_builder.graph.degree())\n",
    "    top_nodes = sorted(node_degrees.items(), key=lambda x: x[1], reverse=True)[:max_nodes]\n",
    "    top_node_names = [node[0] for node in top_nodes]\n",
    "    \n",
    "    subgraph = kg_builder.graph.subgraph(top_node_names)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Create layout\n",
    "    pos = nx.spring_layout(subgraph, k=3, iterations=50)\n",
    "    \n",
    "    # Draw nodes with different colors based on degree\n",
    "    node_colors = []\n",
    "    node_sizes = []\n",
    "    for node in subgraph.nodes():\n",
    "        degree = node_degrees[node]\n",
    "        if degree >= 4:\n",
    "            node_colors.append('red')  # Highly connected\n",
    "            node_sizes.append(1000)\n",
    "        elif degree >= 2:\n",
    "            node_colors.append('orange')  # Moderately connected\n",
    "            node_sizes.append(700)\n",
    "        else:\n",
    "            node_colors.append('lightblue')  # Less connected\n",
    "            node_sizes.append(500)\n",
    "    \n",
    "    # Draw the graph\n",
    "    nx.draw_networkx_nodes(subgraph, pos, node_color=node_colors, \n",
    "                          node_size=node_sizes, alpha=0.8)\n",
    "    nx.draw_networkx_edges(subgraph, pos, edge_color='gray', \n",
    "                          arrows=True, arrowsize=20, alpha=0.6)\n",
    "    nx.draw_networkx_labels(subgraph, pos, font_size=8, font_weight='bold')\n",
    "    \n",
    "    # Add edge labels for relationships\n",
    "    edge_labels = {}\n",
    "    for edge in subgraph.edges(data=True):\n",
    "        relation = edge[2].get('relation', 'related')\n",
    "        edge_labels[(edge[0], edge[1])] = relation\n",
    "    \n",
    "    nx.draw_networkx_edge_labels(subgraph, pos, edge_labels, font_size=6)\n",
    "    \n",
    "    plt.title(f\"Knowledge Graph - Top {max_nodes} Most Connected Entities\", \n",
    "              fontsize=16, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [\n",
    "        plt.scatter([], [], c='red', s=100, label='Highly connected (4+ edges)'),\n",
    "        plt.scatter([], [], c='orange', s=100, label='Moderately connected (2-3 edges)'),\n",
    "        plt.scatter([], [], c='lightblue', s=100, label='Less connected (1 edge)')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\nğŸ“Š Graph Statistics:\")\n",
    "    print(f\"   Total nodes: {subgraph.number_of_nodes()}\")\n",
    "    print(f\"   Total edges: {subgraph.number_of_edges()}\")\n",
    "    print(f\"   Average degree: {np.mean([d for n, d in subgraph.degree()]):.2f}\")\n",
    "    \n",
    "    return subgraph\n",
    "\n",
    "# Visualize the knowledge graph\n",
    "subgraph = visualize_knowledge_graph(kg_builder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph-based Search Implementation\n",
    "\n",
    "Now let's implement different graph-based search methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphBasedSearch:\n",
    "    def __init__(self, kg_builder: KnowledgeGraphBuilder):\n",
    "        self.kg = kg_builder\n",
    "        self.graph = kg_builder.graph\n",
    "        self.entities = kg_builder.entities\n",
    "        \n",
    "    def entity_search(self, query: str, top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"Search for entities matching the query\"\"\"\n",
    "        query_lower = query.lower()\n",
    "        matches = []\n",
    "        \n",
    "        for entity in self.entities:\n",
    "            if query_lower in entity.lower():\n",
    "                score = len(query_lower) / len(entity.lower())  # Simple relevance score\n",
    "                entity_info = self.kg.get_entity_info(entity)\n",
    "                matches.append({\n",
    "                    'entity': entity,\n",
    "                    'score': score,\n",
    "                    'info': entity_info\n",
    "                })\n",
    "        \n",
    "        # Sort by score and connections\n",
    "        matches.sort(key=lambda x: (x['score'], x['info']['connections']), reverse=True)\n",
    "        return matches[:top_k]\n",
    "    \n",
    "    def relationship_search(self, entity: str, max_hops: int = 2) -> Dict:\n",
    "        \"\"\"Find entities related to a given entity within max_hops\"\"\"\n",
    "        if entity not in self.entities:\n",
    "            return {'error': f'Entity \"{entity}\" not found in knowledge graph'}\n",
    "        \n",
    "        related_entities = {}\n",
    "        \n",
    "        # Use BFS to find entities within max_hops\n",
    "        visited = {entity}\n",
    "        queue = [(entity, 0)]  # (entity, hop_count)\n",
    "        \n",
    "        while queue:\n",
    "            current_entity, hops = queue.pop(0)\n",
    "            \n",
    "            if hops < max_hops:\n",
    "                # Get neighbors (both directions)\n",
    "                neighbors = set(self.graph.successors(current_entity)) | set(self.graph.predecessors(current_entity))\n",
    "                \n",
    "                for neighbor in neighbors:\n",
    "                    if neighbor not in visited:\n",
    "                        visited.add(neighbor)\n",
    "                        queue.append((neighbor, hops + 1))\n",
    "                        \n",
    "                        # Get relationship info\n",
    "                        relationship = 'unknown'\n",
    "                        if self.graph.has_edge(current_entity, neighbor):\n",
    "                            relationship = self.graph[current_entity][neighbor].get('relation', 'related_to')\n",
    "                        elif self.graph.has_edge(neighbor, current_entity):\n",
    "                            relationship = self.graph[neighbor][current_entity].get('relation', 'related_to')\n",
    "                        \n",
    "                        if hops + 1 not in related_entities:\n",
    "                            related_entities[hops + 1] = []\n",
    "                        \n",
    "                        related_entities[hops + 1].append({\n",
    "                            'entity': neighbor,\n",
    "                            'relationship': relationship,\n",
    "                            'path_length': hops + 1,\n",
    "                            'via': current_entity if hops > 0 else None\n",
    "                        })\n",
    "        \n",
    "        return {\n",
    "            'query_entity': entity,\n",
    "            'related_entities': related_entities,\n",
    "            'total_found': sum(len(entities) for entities in related_entities.values())\n",
    "        }\n",
    "    \n",
    "    def path_search(self, start_entity: str, end_entity: str) -> List[List[str]]:\n",
    "        \"\"\"Find paths between two entities\"\"\"\n",
    "        if start_entity not in self.entities or end_entity not in self.entities:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # Find shortest paths (up to 3 paths)\n",
    "            paths = []\n",
    "            for path in nx.all_simple_paths(self.graph, start_entity, end_entity, cutoff=4):\n",
    "                if len(paths) < 3:  # Limit to 3 paths\n",
    "                    # Add relationship information\n",
    "                    path_with_relations = []\n",
    "                    for i in range(len(path) - 1):\n",
    "                        relation = self.graph[path[i]][path[i+1]].get('relation', 'related_to')\n",
    "                        path_with_relations.append((path[i], relation, path[i+1]))\n",
    "                    paths.append(path_with_relations)\n",
    "            return paths\n",
    "        except nx.NetworkXNoPath:\n",
    "            return []\n",
    "    \n",
    "    def concept_expansion(self, concepts: List[str]) -> Dict:\n",
    "        \"\"\"Expand a list of concepts using graph relationships\"\"\"\n",
    "        expanded_concepts = set(concepts)\n",
    "        expansion_info = {}\n",
    "        \n",
    "        for concept in concepts:\n",
    "            if concept in self.entities:\n",
    "                # Find related entities\n",
    "                related = self.relationship_search(concept, max_hops=1)\n",
    "                if 'related_entities' in related:\n",
    "                    for hop_level, entities in related['related_entities'].items():\n",
    "                        for entity_info in entities:\n",
    "                            expanded_concepts.add(entity_info['entity'])\n",
    "                expansion_info[concept] = related\n",
    "        \n",
    "        return {\n",
    "            'original_concepts': concepts,\n",
    "            'expanded_concepts': list(expanded_concepts),\n",
    "            'expansion_details': expansion_info,\n",
    "            'expansion_ratio': len(expanded_concepts) / len(concepts)\n",
    "        }\n",
    "\n",
    "# Initialize graph search\n",
    "graph_search = GraphBasedSearch(kg_builder)\n",
    "\n",
    "print(\"ğŸ” Graph-based search initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Demo: Graph-based Search Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Entity Search\n",
    "print(\"ğŸ” Example 1: Entity Search for 'Python'\")\n",
    "print(\"=\" * 50)\n",
    "results = graph_search.entity_search(\"Python\", top_k=3)\n",
    "for result in results:\n",
    "    print(f\"\\nğŸ“Œ Entity: {result['entity']} (Score: {result['score']:.3f})\")\n",
    "    print(f\"   Documents: {result['info']['documents']}\")\n",
    "    print(f\"   Connections: {result['info']['connections']}\")\n",
    "    if result['info']['relationships']:\n",
    "        print(f\"   Relationships:\")\n",
    "        for rel in result['info']['relationships'][:3]:  # Show top 3\n",
    "            if rel['type'] == 'outgoing':\n",
    "                print(f\"     â†’ {rel['relation']} â†’ {rel['target']}\")\n",
    "            else:\n",
    "                print(f\"     â† {rel['relation']} â† {rel['source']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Relationship Search\n",
    "print(\"ğŸ” Example 2: Finding entities related to 'Machine Learning'\")\n",
    "print(\"=\" * 60)\n",
    "related_results = graph_search.relationship_search(\"Machine Learning\", max_hops=2)\n",
    "if 'error' not in related_results:\n",
    "    print(f\"Query Entity: {related_results['query_entity']}\")\n",
    "    print(f\"Total Related Entities Found: {related_results['total_found']}\")\n",
    "    \n",
    "    for hop_level, entities in related_results['related_entities'].items():\n",
    "        print(f\"\\nğŸ”— Hop {hop_level} ({len(entities)} entities):\")\n",
    "        for entity_info in entities[:5]:  # Show top 5 per level\n",
    "            via_text = f\" (via {entity_info['via']})\" if entity_info['via'] else \"\"\n",
    "            print(f\"   - {entity_info['entity']} [{entity_info['relationship']}]{via_text}\")\n",
    "else:\n",
    "    print(related_results['error'])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Path Search\n",
    "print(\"ğŸ” Example 3: Finding paths between 'Python' and 'Computer Vision'\")\n",
    "print(\"=\" * 65)\n",
    "paths = graph_search.path_search(\"Python\", \"Computer Vision\")\n",
    "if paths:\n",
    "    print(f\"Found {len(paths)} path(s):\")\n",
    "    for i, path in enumerate(paths, 1):\n",
    "        print(f\"\\nğŸ›¤ï¸ Path {i}:\")\n",
    "        path_str = path[0][0]  # Start with first entity\n",
    "        for step in path:\n",
    "            path_str += f\" --[{step[1]}]--> {step[2]}\"\n",
    "        print(f\"   {path_str}\")\n",
    "else:\n",
    "    print(\"No paths found between these entities.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Concept Expansion\n",
    "print(\"ğŸ” Example 4: Concept Expansion for ['Deep Learning', 'NLP']\")\n",
    "print(\"=\" * 60)\n",
    "expansion_results = graph_search.concept_expansion([\"Deep Learning\", \"NLP\"])\n",
    "print(f\"Original concepts: {expansion_results['original_concepts']}\")\n",
    "print(f\"Expanded to {len(expansion_results['expanded_concepts'])} concepts\")\n",
    "print(f\"Expansion ratio: {expansion_results['expansion_ratio']:.2f}x\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Expanded Concepts:\")\n",
    "new_concepts = set(expansion_results['expanded_concepts']) - set(expansion_results['original_concepts'])\n",
    "for concept in list(new_concepts)[:8]:  # Show first 8 new concepts\n",
    "    print(f\"   + {concept}\")\n",
    "\n",
    "if len(new_concepts) > 8:\n",
    "    print(f\"   ... and {len(new_concepts) - 8} more\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Embedding-based Search Setup\n",
    "\n",
    "Now let's set up embedding-based search to compare and combine with graph search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Create embeddings for documents\n",
    "documents_text = []\n",
    "document_metadata = []\n",
    "\n",
    "for doc in sample_documents:\n",
    "    full_text = f\"{doc['title']} {doc['content']}\"\n",
    "    documents_text.append(full_text)\n",
    "    document_metadata.append({\n",
    "        'doc_id': doc['id'],\n",
    "        'title': doc['title']\n",
    "    })\n",
    "\n",
    "# Generate embeddings\n",
    "print(\"ğŸ§® Generating embeddings...\")\n",
    "start_time = time.time()\n",
    "document_embeddings = embedding_model.encode(documents_text, show_progress_bar=True)\n",
    "embedding_time = time.time() - start_time\n",
    "\n",
    "print(f\"âœ… Generated {len(document_embeddings)} embeddings in {embedding_time:.2f} seconds\")\n",
    "print(f\"   Embedding dimension: {document_embeddings.shape[1]}\")\n",
    "\n",
    "# Create FAISS index for fast similarity search\n",
    "dimension = document_embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatIP(dimension)  # Inner product for similarity\n",
    "\n",
    "# Normalize embeddings for cosine similarity\n",
    "normalized_embeddings = document_embeddings / np.linalg.norm(document_embeddings, axis=1, keepdims=True)\n",
    "faiss_index.add(normalized_embeddings.astype('float32'))\n",
    "\n",
    "print(f\"ğŸ“Š FAISS index created with {faiss_index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hybrid Search: Combining Graph and Embeddings\n",
    "\n",
    "This is where the magic happens! We'll combine graph-based knowledge with embedding-based semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridKnowledgeSearch:\n",
    "    def __init__(self, graph_search: GraphBasedSearch, embedding_model, faiss_index, \n",
    "                 documents_text: List[str], document_metadata: List[Dict]):\n",
    "        self.graph_search = graph_search\n",
    "        self.embedding_model = embedding_model\n",
    "        self.faiss_index = faiss_index\n",
    "        self.documents_text = documents_text\n",
    "        self.document_metadata = document_metadata\n",
    "    \n",
    "    def embedding_search(self, query: str, top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"Perform embedding-based similarity search\"\"\"\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "        \n",
    "        # Search in FAISS index\n",
    "        scores, indices = self.faiss_index.search(query_embedding.astype('float32'), top_k)\n",
    "        \n",
    "        results = []\n",
    "        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "            results.append({\n",
    "                'rank': i + 1,\n",
    "                'doc_id': self.document_metadata[idx]['doc_id'],\n",
    "                'title': self.document_metadata[idx]['title'],\n",
    "                'content': self.documents_text[idx],\n",
    "                'similarity_score': float(score),\n",
    "                'search_type': 'embedding'\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def graph_enhanced_search(self, query: str, top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"Enhance embedding search with graph knowledge\"\"\"\n",
    "        # Step 1: Find relevant entities in the query\n",
    "        query_entities = self.graph_search.kg.extract_entities(query)\n",
    "        \n",
    "        # Step 2: Expand concepts using graph relationships\n",
    "        if query_entities:\n",
    "            expansion_results = self.graph_search.concept_expansion(query_entities)\n",
    "            expanded_concepts = expansion_results['expanded_concepts']\n",
    "        else:\n",
    "            expanded_concepts = []\n",
    "        \n",
    "        # Step 3: Create enriched query\n",
    "        enriched_query = query\n",
    "        if expanded_concepts:\n",
    "            # Add top related concepts to query\n",
    "            top_concepts = expanded_concepts[:5]  # Limit to avoid query bloat\n",
    "            enriched_query += \" \" + \" \".join(top_concepts)\n",
    "        \n",
    "        # Step 4: Perform embedding search with enriched query\n",
    "        embedding_results = self.embedding_search(enriched_query, top_k)\n",
    "        \n",
    "        # Step 5: Add graph context to results\n",
    "        for result in embedding_results:\n",
    "            result['search_type'] = 'graph_enhanced'\n",
    "            result['query_entities'] = query_entities\n",
    "            result['expanded_concepts'] = expanded_concepts\n",
    "            result['enriched_query'] = enriched_query\n",
    "        \n",
    "        return embedding_results\n",
    "    \n",
    "    def hybrid_search(self, query: str, top_k: int = 5, alpha: float = 0.7) -> List[Dict]:\n",
    "        \"\"\"Combine embedding and graph search with weighted scoring\"\"\"\n",
    "        # Get results from both methods\n",
    "        embedding_results = self.embedding_search(query, top_k * 2)  # Get more candidates\n",
    "        graph_results = self.graph_enhanced_search(query, top_k * 2)\n",
    "        \n",
    "        # Create combined scoring\n",
    "        doc_scores = {}\n",
    "        \n",
    "        # Score from embedding search\n",
    "        for result in embedding_results:\n",
    "            doc_id = result['doc_id']\n",
    "            embedding_score = result['similarity_score']\n",
    "            doc_scores[doc_id] = {\n",
    "                'embedding_score': embedding_score,\n",
    "                'graph_score': 0.0,\n",
    "                'result': result\n",
    "            }\n",
    "        \n",
    "        # Score from graph-enhanced search\n",
    "        for result in graph_results:\n",
    "            doc_id = result['doc_id']\n",
    "            graph_score = result['similarity_score']\n",
    "            if doc_id in doc_scores:\n",
    "                doc_scores[doc_id]['graph_score'] = graph_score\n",
    "            else:\n",
    "                doc_scores[doc_id] = {\n",
    "                    'embedding_score': 0.0,\n",
    "                    'graph_score': graph_score,\n",
    "                    'result': result\n",
    "                }\n",
    "        \n",
    "        # Calculate hybrid scores\n",
    "        hybrid_results = []\n",
    "        for doc_id, scores in doc_scores.items():\n",
    "            # Weighted combination\n",
    "            hybrid_score = (alpha * scores['embedding_score'] + \n",
    "                          (1 - alpha) * scores['graph_score'])\n",
    "            \n",
    "            result = scores['result'].copy()\n",
    "            result.update({\n",
    "                'hybrid_score': hybrid_score,\n",
    "                'embedding_component': scores['embedding_score'],\n",
    "                'graph_component': scores['graph_score'],\n",
    "                'search_type': 'hybrid',\n",
    "                'alpha': alpha\n",
    "            })\n",
    "            hybrid_results.append(result)\n",
    "        \n",
    "        # Sort by hybrid score\n",
    "        hybrid_results.sort(key=lambda x: x['hybrid_score'], reverse=True)\n",
    "        \n",
    "        # Re-rank\n",
    "        for i, result in enumerate(hybrid_results[:top_k], 1):\n",
    "            result['rank'] = i\n",
    "        \n",
    "        return hybrid_results[:top_k]\n",
    "    \n",
    "    def compare_search_methods(self, query: str, top_k: int = 3) -> Dict:\n",
    "        \"\"\"Compare all search methods side by side\"\"\"\n",
    "        start_time = time.time()\n",
    "        embedding_results = self.embedding_search(query, top_k)\n",
    "        embedding_time = time.time() - start_time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        graph_results = self.graph_enhanced_search(query, top_k)\n",
    "        graph_time = time.time() - start_time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        hybrid_results = self.hybrid_search(query, top_k)\n",
    "        hybrid_time = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'embedding_search': {\n",
    "                'results': embedding_results,\n",
    "                'time': embedding_time\n",
    "            },\n",
    "            'graph_enhanced_search': {\n",
    "                'results': graph_results,\n",
    "                'time': graph_time\n",
    "            },\n",
    "            'hybrid_search': {\n",
    "                'results': hybrid_results,\n",
    "                'time': hybrid_time\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Initialize hybrid search\n",
    "hybrid_search = HybridKnowledgeSearch(\n",
    "    graph_search, embedding_model, faiss_index, \n",
    "    documents_text, document_metadata\n",
    ")\n",
    "\n",
    "print(\"ğŸ¯ Hybrid Knowledge Search initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comprehensive Search Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different search methods with various queries\n",
    "test_queries = [\n",
    "    \"Python libraries for machine learning\",\n",
    "    \"Deep learning frameworks\",\n",
    "    \"Data visualization tools\",\n",
    "    \"Computer vision with neural networks\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ”¬ COMPREHENSIVE SEARCH METHOD COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\nğŸ¯ Query {i}: '{query}'\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    comparison = hybrid_search.compare_search_methods(query, top_k=3)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Performance:\")\n",
    "    print(f\"   Embedding Search: {comparison['embedding_search']['time']:.3f}s\")\n",
    "    print(f\"   Graph Enhanced:   {comparison['graph_enhanced_search']['time']:.3f}s\")\n",
    "    print(f\"   Hybrid Search:    {comparison['hybrid_search']['time']:.3f}s\")\n",
    "    \n",
    "    # Show top result from each method\n",
    "    methods = ['embedding_search', 'graph_enhanced_search', 'hybrid_search']\n",
    "    method_names = ['ğŸ” Embedding', 'ğŸ•¸ï¸ Graph Enhanced', 'ğŸ¯ Hybrid']\n",
    "    \n",
    "    for method, method_name in zip(methods, method_names):\n",
    "        results = comparison[method]['results']\n",
    "        if results:\n",
    "            top_result = results[0]\n",
    "            score_info = \"\"\n",
    "            if method == 'hybrid_search':\n",
    "                score_info = f\" (E:{top_result['embedding_component']:.3f}, G:{top_result['graph_component']:.3f})\"\n",
    "            elif 'similarity_score' in top_result:\n",
    "                score_info = f\" ({top_result['similarity_score']:.3f})\"\n",
    "            \n",
    "            print(f\"\\n{method_name} Top Result{score_info}:\")\n",
    "            print(f\"   ğŸ“„ {top_result['title']}\")\n",
    "            \n",
    "            # Show additional context for graph-enhanced search\n",
    "            if method == 'graph_enhanced_search' and 'expanded_concepts' in top_result:\n",
    "                concepts = top_result['expanded_concepts'][:3]\n",
    "                if concepts:\n",
    "                    print(f\"   ğŸ§  Expanded concepts: {', '.join(concepts)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Search Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_search_demo(query: str):\n",
    "    \"\"\"Interactive demonstration of all search capabilities\"\"\"\n",
    "    print(f\"ğŸ” INTERACTIVE SEARCH DEMO\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 1. Entity Recognition in Query\n",
    "    query_entities = graph_search.kg.extract_entities(query)\n",
    "    print(f\"\\nğŸ·ï¸ Entities detected in query: {query_entities if query_entities else 'None'}\")\n",
    "    \n",
    "    # 2. Concept Expansion\n",
    "    if query_entities:\n",
    "        expansion = graph_search.concept_expansion(query_entities[:2])  # Limit to 2 entities\n",
    "        expanded_concepts = list(set(expansion['expanded_concepts']) - set(query_entities))\n",
    "        print(f\"\\nğŸ§  Concept expansion found {len(expanded_concepts)} related concepts:\")\n",
    "        for concept in expanded_concepts[:6]:\n",
    "            print(f\"   + {concept}\")\n",
    "    \n",
    "    # 3. Hybrid Search Results\n",
    "    print(f\"\\nğŸ¯ Hybrid Search Results:\")\n",
    "    hybrid_results = hybrid_search.hybrid_search(query, top_k=4)\n",
    "    \n",
    "    for i, result in enumerate(hybrid_results, 1):\n",
    "        print(f\"\\n   {i}. {result['title']}\")\n",
    "        print(f\"      ğŸ“Š Hybrid Score: {result['hybrid_score']:.3f}\")\n",
    "        print(f\"      ğŸ” Embedding: {result['embedding_component']:.3f} | \"\n",
    "              f\"ğŸ•¸ï¸ Graph: {result['graph_component']:.3f}\")\n",
    "        \n",
    "        # Show snippet\n",
    "        content = result['content']\n",
    "        if len(content) > 150:\n",
    "            content = content[:150] + \"...\"\n",
    "        print(f\"      ğŸ“ {content}\")\n",
    "    \n",
    "    # 4. Graph Relationships (if entities found)\n",
    "    if query_entities:\n",
    "        print(f\"\\nğŸ•¸ï¸ Graph Relationships for '{query_entities[0]}':\")\n",
    "        entity_info = graph_search.kg.get_entity_info(query_entities[0])\n",
    "        if entity_info and entity_info['relationships']:\n",
    "            for rel in entity_info['relationships'][:4]:\n",
    "                if rel['type'] == 'outgoing':\n",
    "                    print(f\"      {query_entities[0]} --[{rel['relation']}]--> {rel['target']}\")\n",
    "                else:\n",
    "                    print(f\"      {rel['source']} --[{rel['relation']}]--> {query_entities[0]}\")\n",
    "        else:\n",
    "            print(f\"      No relationships found\")\n",
    "\n",
    "# Test the interactive demo\n",
    "demo_queries = [\n",
    "    \"How to use Python for deep learning?\",\n",
    "    \"Computer vision with TensorFlow\"\n",
    "]\n",
    "\n",
    "for query in demo_queries:\n",
    "    interactive_search_demo(query)\n",
    "    print(\"\\n\" + \"=\" * 100 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_search_performance():\n",
    "    \"\"\"Analyze and compare search method performance\"\"\"\n",
    "    test_queries = [\n",
    "        \"machine learning algorithms\",\n",
    "        \"python data science\",\n",
    "        \"neural networks\",\n",
    "        \"visualization libraries\",\n",
    "        \"deep learning frameworks\"\n",
    "    ]\n",
    "    \n",
    "    results = {\n",
    "        'embedding': {'times': [], 'avg_scores': []},\n",
    "        'graph_enhanced': {'times': [], 'avg_scores': []},\n",
    "        'hybrid': {'times': [], 'avg_scores': []}\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ“Š PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for query in test_queries:\n",
    "        comparison = hybrid_search.compare_search_methods(query, top_k=3)\n",
    "        \n",
    "        # Collect timing data\n",
    "        results['embedding']['times'].append(comparison['embedding_search']['time'])\n",
    "        results['graph_enhanced']['times'].append(comparison['graph_enhanced_search']['time'])\n",
    "        results['hybrid']['times'].append(comparison['hybrid_search']['time'])\n",
    "        \n",
    "        # Collect average scores\n",
    "        if comparison['embedding_search']['results']:\n",
    "            avg_emb_score = np.mean([r['similarity_score'] for r in comparison['embedding_search']['results']])\n",
    "            results['embedding']['avg_scores'].append(avg_emb_score)\n",
    "        \n",
    "        if comparison['graph_enhanced_search']['results']:\n",
    "            avg_graph_score = np.mean([r['similarity_score'] for r in comparison['graph_enhanced_search']['results']])\n",
    "            results['graph_enhanced']['avg_scores'].append(avg_graph_score)\n",
    "        \n",
    "        if comparison['hybrid_search']['results']:\n",
    "            avg_hybrid_score = np.mean([r['hybrid_score'] for r in comparison['hybrid_search']['results']])\n",
    "            results['hybrid']['avg_scores'].append(avg_hybrid_score)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    print(f\"\\nâ±ï¸ Average Response Times:\")\n",
    "    for method, data in results.items():\n",
    "        avg_time = np.mean(data['times'])\n",
    "        std_time = np.std(data['times'])\n",
    "        print(f\"   {method.replace('_', ' ').title():15}: {avg_time:.4f}s (Â±{std_time:.4f}s)\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ Average Relevance Scores:\")\n",
    "    for method, data in results.items():\n",
    "        if data['avg_scores']:\n",
    "            avg_score = np.mean(data['avg_scores'])\n",
    "            std_score = np.std(data['avg_scores'])\n",
    "            print(f\"   {method.replace('_', ' ').title():15}: {avg_score:.4f} (Â±{std_score:.4f})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run performance analysis\n",
    "performance_results = analyze_search_performance()\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Subplot 1: Response Times\n",
    "plt.subplot(1, 2, 1)\n",
    "methods = ['Embedding', 'Graph Enhanced', 'Hybrid']\n",
    "times = [np.mean(performance_results['embedding']['times']),\n",
    "         np.mean(performance_results['graph_enhanced']['times']),\n",
    "         np.mean(performance_results['hybrid']['times'])]\n",
    "errors = [np.std(performance_results['embedding']['times']),\n",
    "          np.std(performance_results['graph_enhanced']['times']),\n",
    "          np.std(performance_results['hybrid']['times'])]\n",
    "\n",
    "bars = plt.bar(methods, times, yerr=errors, capsize=5, \n",
    "               color=['skyblue', 'lightgreen', 'orange'], alpha=0.7)\n",
    "plt.title('Average Response Time by Search Method')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, time in zip(bars, times):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "             f'{time:.4f}s', ha='center', va='bottom')\n",
    "\n",
    "# Subplot 2: Score Distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "scores = [performance_results['embedding']['avg_scores'],\n",
    "          performance_results['graph_enhanced']['avg_scores'],\n",
    "          performance_results['hybrid']['avg_scores']]\n",
    "labels = ['Embedding', 'Graph Enhanced', 'Hybrid']\n",
    "colors = ['skyblue', 'lightgreen', 'orange']\n",
    "\n",
    "bp = plt.boxplot(scores, labels=labels, patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "plt.title('Relevance Score Distribution')\n",
    "plt.ylabel('Relevance Score')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Key Insights\n",
    "\n",
    "### ğŸ¯ What We've Built:\n",
    "\n",
    "1. **Graph-based Knowledge Graph**: Extracted entities and relationships from documents\n",
    "2. **Multiple Search Methods**: Entity search, relationship traversal, path finding, concept expansion\n",
    "3. **Embedding-based Search**: Fast semantic similarity search using sentence transformers\n",
    "4. **Hybrid Approach**: Combined graph knowledge with embedding similarity for enhanced results\n",
    "\n",
    "### ğŸ” Search Method Comparison:\n",
    "\n",
    "| Method | Strengths | Weaknesses | Best Use Cases |\n",
    "|--------|-----------|------------|----------------|\n",
    "| **Embedding Search** | Fast, semantic understanding, good for general queries | No explicit relationships, may miss domain connections | General semantic search, content similarity |\n",
    "| **Graph Search** | Explicit relationships, reasoning capabilities, domain knowledge | Requires good entity extraction, limited by graph coverage | Domain-specific queries, relationship exploration |\n",
    "| **Hybrid Search** | Best of both worlds, enhanced relevance, context-aware | More complex, slower than pure embedding | Complex queries requiring both semantic and structural understanding |\n",
    "\n",
    "### ğŸ’¡ Key Benefits of Graph + Embeddings:\n",
    "\n",
    "1. **Enhanced Query Understanding**: Graph concepts expand query context\n",
    "2. **Better Relevance**: Combining structural and semantic signals\n",
    "3. **Explainable Results**: Graph relationships provide reasoning trails\n",
    "4. **Domain Adaptation**: Graph captures domain-specific relationships\n",
    "5. **Query Expansion**: Automatic concept expansion using graph relationships\n",
    "\n",
    "### ğŸš€ Next Steps:\n",
    "\n",
    "- **Larger Knowledge Graphs**: Use more sophisticated entity extraction (spaCy, BERT-NER)\n",
    "- **Dynamic Updates**: Real-time graph updates as new documents are added\n",
    "- **Graph Embeddings**: Combine node2vec or GraphSAGE with document embeddings\n",
    "- **Multi-modal**: Extend to images, videos with visual knowledge graphs\n",
    "- **Federated Search**: Combine multiple knowledge sources and graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final demonstration: Show the complete pipeline\n",
    "def complete_pipeline_demo(user_query: str):\n",
    "    \"\"\"Demonstrate the complete pipeline from query to results\"\"\"\n",
    "    print(f\"ğŸ”„ COMPLETE PIPELINE DEMONSTRATION\")\n",
    "    print(f\"User Query: '{user_query}'\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Step 1: Entity extraction\n",
    "    entities = graph_search.kg.extract_entities(user_query)\n",
    "    print(f\"\\n1ï¸âƒ£ Entity Extraction: {entities}\")\n",
    "    \n",
    "    # Step 2: Graph expansion\n",
    "    if entities:\n",
    "        expansion = graph_search.concept_expansion(entities[:2])\n",
    "        new_concepts = list(set(expansion['expanded_concepts']) - set(entities))\n",
    "        print(f\"\\n2ï¸âƒ£ Graph Expansion: +{len(new_concepts)} concepts\")\n",
    "        print(f\"   Added: {', '.join(new_concepts[:5])}{'...' if len(new_concepts) > 5 else ''}\")\n",
    "    \n",
    "    # Step 3: Embedding generation\n",
    "    print(f\"\\n3ï¸âƒ£ Query Embedding: Generated {embedding_model.encode([user_query]).shape[1]}D vector\")\n",
    "    \n",
    "    # Step 4: Hybrid search\n",
    "    results = hybrid_search.hybrid_search(user_query, top_k=3)\n",
    "    print(f\"\\n4ï¸âƒ£ Hybrid Search Results:\")\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n   ğŸ“„ {i}. {result['title']}\")\n",
    "        print(f\"      ğŸ¯ Final Score: {result['hybrid_score']:.4f}\")\n",
    "        print(f\"      ğŸ“Š Components: Embedding({result['embedding_component']:.3f}) + \"\n",
    "              f\"Graph({result['graph_component']:.3f})\")\n",
    "    \n",
    "    print(f\"\\nâœ… Pipeline completed successfully!\")\n",
    "    return results\n",
    "\n",
    "# Demo with a complex query\n",
    "demo_query = \"What are the best Python frameworks for building computer vision applications?\"\n",
    "final_results = complete_pipeline_demo(demo_query)\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(f\"ğŸ“š NOTEBOOK COMPLETE! You now have a comprehensive understanding of:\")\n",
    "print(f\"   ğŸ•¸ï¸ Graph-based Knowledge Graphs\")\n",
    "print(f\"   ğŸ” Multiple search methodologies\")\n",
    "print(f\"   ğŸ¯ Hybrid embedding + graph approaches\")\n",
    "print(f\"   ğŸ“Š Performance comparison and analysis\")\n",
    "print(f\"\\nğŸš€ Ready to build your own hybrid knowledge search systems!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGUdemy (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
